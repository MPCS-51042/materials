{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made a test runner that kinda works. Buuut....\n",
    "\n",
    "- What if we have several different subclasses of Test and we want to run all the tests at once?\n",
    "- What if we have some code that we need to run before and after every test?\n",
    "- What if we want to be able to skip a test, or only run specific tests according to some rule we made up?\n",
    "\n",
    "It is time for us to make these dreams come to life! But first, let's import the matchers we worked on last time so we can keep using them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix_test.matchers import FailedAssertion, Assertion, assert_that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also import colorama, because we've made the test runner print in colors, and who doesn't love colors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install colorama \n",
    "\n",
    "from colorama import Fore, Back, Style "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, here's where we got to with the test runner last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test():\n",
    "    # Runs all the test methods. HOW?!?!\n",
    "    def run(self):\n",
    "        run_count = 0\n",
    "        pass_count = 0\n",
    "        test_methods = [\n",
    "            token for token in dir(self) \\\n",
    "            if token.startswith(\"test\")  \\\n",
    "            and callable(getattr(self.__class__, token))\n",
    "        ]\n",
    "        for method in test_methods:\n",
    "            run_count += 1\n",
    "            try:\n",
    "                getattr(self.__class__, method).__call__(self)\n",
    "                pass_count += 1\n",
    "                print(Fore.GREEN + f\"{method} passed!\")\n",
    "            except Exception as e:\n",
    "                print(Fore.RED +f\"{method}:  {e}\") \n",
    "        print(Style.RESET_ALL)\n",
    "        print(f\"{pass_count} out of {run_count} tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And when we run it on some example tests, we get some beautiful test output. Here, give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_twos(one, two):\n",
    "    return []\n",
    "\n",
    "class FindTwosTest(Test):\n",
    "\n",
    "    def test_empty_inputs(self):\n",
    "        assert_that(find_twos(\"\", \"\")).equals([])\n",
    "        assert_that(find_twos(\"2\", \"\")).equals([])\n",
    "        assert_that(find_twos(\"2\", \"\")).equals([])\n",
    "\n",
    "    def test_non_matching_sets(self):\n",
    "        assert_that(find_twos(\"1\", \"1, 3\")).equals([])\n",
    "\n",
    "    def test_non_matching_twos(self):\n",
    "        assert_that(find_twos(\"2\", \"1, 3\")).equals([])\n",
    "        \n",
    "    def test_matches(self):\n",
    "        assert_that(find_twos(\"12\", \"2, 12\")).equals([12])\n",
    "        assert_that(find_twos(\"1, 2, 20, 22, 44, 99\", \"3, 5, 22, 100, 44, 2\")).equals([2, 22])\n",
    "        \n",
    "FindTwosTest().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is where we are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge:\n",
    "\n",
    "What if I have several different subclasses of Test and I want to run all my tests at once? So, in addition to my `FindTwosTest`, I also want to run this spiffy test suite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SortedTests(Test):\n",
    "    def test_sort_integers(self):\n",
    "        assert_that(sorted([3, 1, 2])).equals([1, 2, 3])\n",
    "\n",
    "    def test_sort_strings(self):\n",
    "        assert_that(sorted([\"C\", \"A\", \"B\"])).equals([\"A\", \"B\", \"C\"])\n",
    "\n",
    "SortedTests().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: make this statement run all the tests:\n",
    "\n",
    "`Test.run_all()`\n",
    "\n",
    "Such that, whichever classes have subclassed `Test`, they all run when you run that command. The output might look something like this:\n",
    "\n",
    "![](../images/run_all_output.png)\n",
    "\n",
    "Hint: use `__init_subclass__`. This method is run on a superclass every time a subclass gets instantiated. You can use it to register all of the subclasses in a collection at the superclass level. You can read all about exactly how this method works in [the PEP proposal](https://www.python.org/dev/peps/pep-0487/) that introduced it to Python!\n",
    "\n",
    "Here is our Test class so far with some starter code added for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test():\n",
    "    types = []\n",
    "    \n",
    "    def __init_subclass__(cls, **kwargs):\n",
    "        super().__init_subclass__(**kwargs)\n",
    "        # Add the initialized class to the types list here!\n",
    "    \n",
    "    @classmethod\n",
    "    def run_all(cls):\n",
    "        pass\n",
    "        # What do we do inside here?\n",
    "\n",
    "    def run(self):\n",
    "        run_count = 0\n",
    "        pass_count = 0\n",
    "        test_methods = [\n",
    "            token for token in dir(self) \\\n",
    "            if token.startswith(\"test\")  \\\n",
    "            and callable(getattr(self.__class__, token))\n",
    "        ]\n",
    "        for method in test_methods:\n",
    "            run_count += 1\n",
    "            try:\n",
    "                getattr(self.__class__, method).__call__(self)\n",
    "                pass_count += 1\n",
    "                print(Fore.GREEN + f\"    {method} passed!\")\n",
    "            except Exception as e:\n",
    "                print(Fore.RED +f\"    {method}:  {e}\") \n",
    "        print(Style.RESET_ALL + f\"    {pass_count} out of {run_count} tests passed.\\n\")\n",
    "        return pass_count, run_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are all our test classes together, so you can easily check whether all of your test methods are getting run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FindTwosTest(Test):\n",
    "\n",
    "    def test_empty_inputs(self):\n",
    "        assert_that(find_twos(\"\", \"\")).equals([])\n",
    "        assert_that(find_twos(\"2\", \"\")).equals([])\n",
    "        assert_that(find_twos(\"2\", \"\")).equals([])\n",
    "\n",
    "    def test_non_matching_sets(self):\n",
    "        assert_that(find_twos(\"1\", \"1, 3\")).equals([])\n",
    "\n",
    "    def test_non_matching_twos(self):\n",
    "        assert_that(find_twos(\"2\", \"1, 3\")).equals([])\n",
    "        \n",
    "    def test_matches(self):\n",
    "        assert_that(find_twos(\"12\", \"2, 12\")).equals([12])\n",
    "        assert_that(find_twos(\"1, 2, 20, 22, 44, 99\", \"3, 5, 22, 100, 44, 2\")).equals([2, 22])\n",
    "        \n",
    "class SortedTests(Test):\n",
    "    def test_sort_integers(self):\n",
    "        assert_that(sorted([3, 1, 2])).equals([1, 2, 3])\n",
    "\n",
    "    def test_sort_strings(self):\n",
    "        assert_that(sorted([\"C\", \"A\", \"B\"])).equals([\"A\", \"B\", \"C\"])\n",
    "\n",
    "Test.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: \n",
    "\n",
    "1. Make the test suite print the name of each test class before running it\n",
    "2. Make the test suite print a total number of tests run and passed\n",
    "\n",
    "**Something like this:**\n",
    "\n",
    "![](../images/test_output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: \n",
    "\n",
    "What if I have some code that I need to run before and after every test? Suppose, for example, that I am keeping a cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedStringQueue:\n",
    "    items = []\n",
    "    \n",
    "    @classmethod\n",
    "    def validate(cls):\n",
    "        invalids = 0\n",
    "        for item in CachedStringQueue.items:\n",
    "            if item == 'invalid':\n",
    "                invalids += 1\n",
    "        cls.items = sorted(cls.items)[invalids:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I have two test suites for the cache: one that checks how to put things in it (assuming an empty cache) and one that checks how to invalidate items (which requires some stuff to be in the cache.)\n",
    "\n",
    "Perhaps we could include `setup` and `teardown` methods in our test class that run before and after each separate test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheSetAndFetchTests(Test):\n",
    "    def test_set_and_fetch(self):\n",
    "        CachedStringQueue.items.append(\"stringo\")\n",
    "        assert_that(CachedStringQueue.items.pop(0)).equals(\"stringo\")\n",
    "    \n",
    "class CacheValidationTests(Test):\n",
    "    def setup(self):\n",
    "        print(\"SETUP RUN\")\n",
    "        CachedStringQueue.items = ['valid', 'invalid', 'valid']\n",
    "    \n",
    "    def teardown(self):\n",
    "        print(\"TEARDOWN RUN\")\n",
    "        CachedStringQueue.items = []\n",
    "        \n",
    "    def test_validation(self):\n",
    "        CachedStringQueue.validate()\n",
    "        assert_that(CachedStringQueue.items).has_size(2)\n",
    "        assert_that(set(CachedStringQueue.items)).equals({'valid'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, those functions are never run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CacheSetAndFetchTests().run()\n",
    "CacheValidationTests().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if they _were_ run at the appropriate times, we'd expect to see some output like this (don't worry about the colors of the prints for now): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/setup_teardown.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge:\n",
    "\n",
    "Add this `setup` and `teardown` function to our test functionality. Remember to call those methods in the test runner to run before and after each test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test():\n",
    "    types = []\n",
    "    \n",
    "    def __init_subclass__(cls, **kwargs):\n",
    "        super().__init_subclass__(**kwargs)\n",
    "        cls.types.append(cls)\n",
    "    \n",
    "    @classmethod\n",
    "    def run_all(cls):\n",
    "        pass_count = 0\n",
    "        run_count = 0\n",
    "        \n",
    "        for typ in cls.types:\n",
    "            print(f\"Running {typ.__name__}: \")\n",
    "            passed, runned = typ().run() \n",
    "            pass_count += passed\n",
    "            run_count += runned\n",
    "        \n",
    "        print(f\"{pass_count} out of {run_count} tests passed.\\n\")\n",
    "    \n",
    "        \n",
    "    def run(self):\n",
    "        run_count = 0\n",
    "        pass_count = 0\n",
    "        test_methods = [\n",
    "            token for token in dir(self) \\\n",
    "            if token.startswith(\"test\")  \\\n",
    "            and callable(getattr(self.__class__, token))\n",
    "        ]\n",
    "        for method in test_methods:\n",
    "            run_count += 1\n",
    "            try:\n",
    "                getattr(self.__class__, method).__call__(self)\n",
    "                pass_count += 1\n",
    "                print(Fore.GREEN + f\"    {method} passed!\")\n",
    "            except Exception as e:\n",
    "                print(Fore.RED +f\"    {method}:  {e}\") \n",
    "        print(Style.RESET_ALL + f\"    {pass_count} out of {run_count} tests passed.\\n\")\n",
    "        return pass_count, run_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CacheSetAndFetchTests().run()\n",
    "CacheValidationTests().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: \n",
    "\n",
    "What if I want to be able to only run specific tests according to some rule I made up?\n",
    "\n",
    "Maybe:\n",
    "\n",
    "`Test.run_all(only=\"match\")` only runs tests with \"match\" in their name. You can modify this version of the test runner to make that happen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test():\n",
    "    types = []\n",
    "    \n",
    "    def __init_subclass__(cls, **kwargs):\n",
    "        super().__init_subclass__(**kwargs)\n",
    "        cls.types.append(cls)\n",
    "    \n",
    "    @classmethod\n",
    "    def run_all(cls):\n",
    "        pass_count = 0\n",
    "        run_count = 0\n",
    "        \n",
    "        for typ in cls.types:\n",
    "            print(f\"Running {typ.__name__}: \")\n",
    "            passed, runned = typ().run() \n",
    "            pass_count += passed\n",
    "            run_count += runned\n",
    "        \n",
    "        print(f\"{pass_count} out of {run_count} tests passed.\\n\")\n",
    "    \n",
    "    def setup(self):\n",
    "        pass\n",
    "    \n",
    "    def teardown(self):\n",
    "        pass\n",
    "        \n",
    "    def run(self):\n",
    "        run_count = 0\n",
    "        pass_count = 0\n",
    "        test_methods = [\n",
    "            token for token in dir(self) \\\n",
    "            if token.startswith(\"test\")  \\\n",
    "            and callable(getattr(self.__class__, token))\n",
    "        ]\n",
    "        for method in test_methods:\n",
    "            self.setup()\n",
    "            run_count += 1\n",
    "            try:\n",
    "                getattr(self.__class__, method).__call__(self)\n",
    "                pass_count += 1\n",
    "                print(Fore.GREEN + f\"    {method} passed!\")\n",
    "            except Exception as e:\n",
    "                print(Fore.RED +f\"    {method}:  {e}\") \n",
    "            self.teardown()\n",
    "        print(Style.RESET_ALL + f\"    {pass_count} out of {run_count} tests passed.\\n\")\n",
    "        return pass_count, run_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we'll _just_ run `FindTwosTest` to keep our example small while making sure that our code works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FindTwosTest(Test):\n",
    "\n",
    "    def test_empty_inputs(self):\n",
    "        assert_that(find_twos(\"\", \"\")).equals([])\n",
    "        assert_that(find_twos(\"2\", \"\")).equals([])\n",
    "        assert_that(find_twos(\"2\", \"\")).equals([])\n",
    "\n",
    "    def test_non_matching_sets(self):\n",
    "        assert_that(find_twos(\"1\", \"1, 3\")).equals([])\n",
    "\n",
    "    def test_non_matching_twos(self):\n",
    "        assert_that(find_twos(\"2\", \"1, 3\")).equals([])\n",
    "        \n",
    "    def test_matches(self):\n",
    "        assert_that(find_twos(\"12\", \"2, 12\")).equals([12])\n",
    "        assert_that(find_twos(\"1, 2, 20, 22, 44, 99\", \"3, 5, 22, 100, 44, 2\")).equals([2, 22])\n",
    "\n",
    "Test.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test.run_all(only=\"match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
